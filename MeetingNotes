report draft: https://docs.google.com/document/d/1-e0O9ZkiwhUkSayUI4P8e1tfwYXRhIO96kZ4xzZloSw/edit?usp=sharing

24.06.2019
* design intuitive and functional GUI
* visualisation of multiple parameters (multidimensional representation of data) 
  - graphical, 3D, text? [maybe 3D graph + point shape + colour?]
* write the use case
* create survey for sound designers on their expectations on the GUI
  - make wireframes
  - get ethics form from moodle
  
Plan
  * find the right synthesizer 
  * find the best way of communicating btween the synth and python
  * get the ML code
  
1.07.2019
Meeting
* defining sound samples - spectrogram > average in time > vector - distance between them
* main problem of the project - how to call the synthesizer
* 3D representation - linear or non-linear interpolation
* PCA
* GUI:
  - data visualisation + presets memory + crossfading (could have)
  - give the user the ability to customise data visualisation (colours, shapes, relations)?
 * remember to focus on the softdev aspect of the project
 
 [Email: vector representation:https://librosa.github.io/librosa/ .
 It can be used to compute the Mel frequency cepstral coefficients (mfccs)
 https://librosa.github.io/librosa/feature.html#feature (a matrix),
 and you can then take the mean across the temporal axis to get a vector.]

Plan
* analyse: https://github.com/akuhren/target_vector_estimation
* find weather recordings
* add more use cases - use MOSCOW
* write the first chapter of the report
* edit the synthesizer to support OSC

6.07.2019 - notes from Anders:

I actually suggest not using the code from our paper, at least initially.
Instead consider emukit (https://github.com/amzn/emukit) which has a great Bayesian Optimisation API.
The gist of it is summarised in this tutorial: https://amzn.github.io/emukit/bayesian-optimization/.

For your use case you’d want to define the cost function as the dissimilarity between the current output
of the synth and the target sound. How you go about that is up to you, but in the experiments for the paper
I did the following:

  1. Produce a 1 second sound from the synth and get it as a NumPy array 
(render to disk and load in Python, or stream directly into the Python process).
  2. Convert the raw waveform to a spectral representation, e.g. DFT or MFCC using librosa
(https://librosa.github.io/librosa/).
  3. (Optional): Project down into 5-10 dimensions using e.g. PCA or something similar
from scikit-learn (https://scikit-learn.org/stable/modules/decomposition.html).
  4. Take the Euclidean distance between the two sounds (produced and target) 
in this representation space and return that as the cost for given the set of synthesizer parameters.

The Bayesian Optimisation framework will then do its magic to find the parameters which minimises that cost,
hopefully leading to a reconstruction of the target sound. If you get it to work with the “standard” API
from emukit we can try and replace it with the code from our paper — it should be fairly straightforward at that point.

8.07.2019
* use PCA to reduce deminesions 
* DFT + PCA > Compressed version of the spectrogram
* how to measure the quality? Percieved sound versus produced
* we need to throw away some data but not too much



